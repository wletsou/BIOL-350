# Association testing {#Week3}

In this assignment, we will use the kinship matrix and principal components derived from our dataset to fit a generalized linear model using the method of restricted maximum likelihood [@] to find the association of a polymorphism with the disease phenotype.

## Statistics {#Week3_theory}

### The odds ratio {#Week3_theory_or}

The logic of any genetic association study is to see if an allele is enriched in subjects affected with disease.&nbsp; In other words, we want to see if the allele is *associated* with disease.&nbsp; We do this by collecting may individuals with disease and a similar number of healthy controls from the general popuation.&nbsp; An individual's risk is its inherited (unobserved) probability $p=P\left(\text{Disease}\mid\text{Allele}\right)$ of developing disease over its lifetime [@falconer_inheritance_1965], and the *relative risk* or RR is the ratio of the risk to carriers to the risk to non-carriers of the allele.&nbsp; However, the RR is difficult to measure prosepctively because it involves waiting a long time for a potentially smaller number of cases to develop.&nbsp; If we retrospectively select cases that have already developed and match them with healthy controls, we can instead calculate an individual's probability $P\left(\text{Allele}\mid\text{Disease}\right)$ of carrying the allele conditioned on its disease status.&nbsp; Related to the probability $p$ of an event is the *odds* $\frac{p}{1-p}$ of the the event, and it turns out that the *odds ratio*
\begin{equation}
\text{OR}=\frac{\frac{P\left(\text{Allele}\mid\text{Cases}\right)}{1-P\left(\text{Allele}\mid\text{Cases}\right)}}{\frac{P\left(\text{Allele}\mid\text{Controls}\right)}{1-P\left(\text{Allele}\mid\text{Controls}\right)}}
(\#eq:3-1)
\end{equation}is invariant to whether we collect subjects prospectively or retrospectively.&nbsp; Hence we use the OR as a convenient measure of the effect of an allele on disease risk in case-control studies.&nbsp; However, the crude measure \@ref(eq:3-1) cannot be adjusted for other factors like sex, age, and genetic ancestry which may also have an effect on disease risk.&nbsp; For this type of analysis, we need logistic regression.

### The logistic model of disease risk {#Week3_theory_logit}

Normally we test the association between two variables using linear regression, but doing so requires that both the dependent and independent variables be continuous.&nbsp; In genetic association studies, neither the outcome (disease) nor the predictor (number of risk alleles) is continuous.&nbsp; However, an individual's unobserved propensity $p$ is a continuous variable that ranges between 0 and 1; furthermore, the individual's *log-odds* $\log{\frac{p}{1-p}}$ of disease is a continuous value that ranges between $-\infty$ and $+\infty$.&nbsp; Thus if we assume that the log-odds of disease can be represented by the equation
\begin{equation}
\log{\frac{p}{1-p}}=\beta_0+\beta_1X_1,
(\#eq:3-2)
\end{equation} where $X_1$ is number of risk alleles and $\beta_1$ is the *log-odds-ratio*, then we can in principle fit a line and estimate its slope.&nbsp; This slope $\beta_1$ would then be interpretted as the multiplicative increase in the log-odds of disease.

Now, since we cannot observe $p_i$ for each subject $i$, we cannot actually fit \@ref(eq:3-2) using linear regression.&nbsp;  We can, however, use the concept of *maximum likelihood*.&nbsp; In statistics, the likelihood of a disease model like \@ref(eq:3-2) is the probability of the data being generated by the model, or
\begin{equation}
\mathcal{L}\left(\text{Model}\mid\text{Data}\right)=P\left(\text{Data}\mid\text{Model}\right).
(\#eq:3-3)
\end{equation}Here the model is the set of parameters $\beta_0,\beta_1$ required to predict disease risk, and we can find estimates for the parameters by maximizing \@ref(eq:3-3), i.e., by finding the model most consistent with the data.&nbsp; Furthermore, if the likelihood function has approximately the shape of a normal distribution, then we can estimate the statistical significance of our estimates by computing their standard error, got from the *curvature* or second derivative of $\mathcal{L}$ near the $\beta$ which maximize it.

The binomial distribution is a good approximation to the normal distribution, so if we model the likelihood of the observed data as
\begin{equation}
\mathcal{L}=\prod_ip_i^{y_i}\left(1-p_i\right)^{1-y_i},
(\#eq:3-4)
\end{equation} where $y_i=0,1$ is an indicator of disease status, then the *log-likelihood* is
\begin{align}
\ell&=\sum_iy_i\log{p_i}+\left(1-y_i\right)\log{\left(1-p_i\right)}\\&=\sum_iy_i\log{\frac{p_i}{1-p_i}}+\log{\left(1-p_i\right)}
(\#eq:3-5)
\end{align}using \@ref(eq:3-2)
\begin{equation}
\ell=\sum_iy_i\left(\beta_0+\beta_1X_{i1}\right)-\log{\left(1+e^{\beta_0+\beta_1X_{i1}}\right)}.
(\#eq:3-6)
\end{equation}Eq. \@ref(eq:3-6) is a function of the parameter $\beta_1$.&nbsp; Thus we can find the *maximum-likelihood estimate* $\hat{\beta_1}$ of $\beta_1$ by solving $\frac{\partial \ell}{\partial \beta_1}\bigr\rvert_{\beta_1=\hat{\beta_1}}$ for $\hat{\beta_1}$ and getting its standard error $\frac{-1}{\frac{\partial^2\ell}{\partial \beta_1^2}\bigr\rvert_{\beta_1=\hat{\beta_1}}}$ by evaluating the curvature of the log-likelihood at the best estimate of $\beta_1$.&nbsp; From these we can also get an estimate of the statistical significance.

#### Simulating phenotypes {#Week3_theory_logit_simulation}

The <kbd>PhenotypeSimulator</kbd> package [@meyer_phenotypesimulator_2018] used to generate the phenotype data used in this assignment generates a vector $\theta$ of log-odds for each individual based on its genotype, given an input list of risk SNPs.&nbsp; To go from the log-odds scale scale to the binary disease scale, we can rearrange the equation $\log\frac{p}{1-p}=\theta$ to generate a vector
\begin{equation}
\mathbb{P}\left(\text{Disease}\mid \theta\right)=\frac{e^{\theta}}{1+e^{\theta}}
(\#eq:3-7)
\end{equation}of disease probabilities.&nbsp; In a balanced case-control study, the probability of disease should be approximately 50%, corresponding to a mean log-odds of $\overline{\theta}=0$.&nbsp; If this condition does not obtain, we can recenter the data so that
\begin{equation}
\mathbb{P}\left(\text{Disease}\mid \theta\right)=\frac{e^{\theta -\overline{\theta}}}{1+e^{\theta-\overline{\theta}}}.
(\#eq:3-8)
\end{equation}This is always possible (in simulated data) because the intercept term $\beta_0$ in Eq. \@ref(eq:3-2) can be shifted without changing the odds ratio $\beta_1$.&nbsp; Disease phenotype can then be generated by drawing $N$ random numbers $t_i$ uniformly from the interval $\left[0,1\right]$ and calling everyone a case whose probability $p_i\geq t_i$.

### Linear mixed models {#Week3_theory_glm}

In genome-wide association studies (GWAS) we want to estimate the odds ratio $e^{\beta_1}$ for each SNP to see if any SNPs are associated with disease.&nbsp; However, there are millions of SNPs and only thousands of subjects, so we cannot fit all the parameters simultaneously.&nbsp; Instead, one SNP effect $\beta_1$ is fit at a time, together with the other *fixed* covariate effects $\beta_j$ against a background of the composite, *random* effects of all the remaining SNPs together, so that our model has two components:
\begin{equation}
Y_i=\log{\frac{p_i}{1-p_i}}=\sum_jX_{ij}\beta_j+\sum_jZ_{ij}u_j+\varepsilon_i.
(\#eq:3-9)
\end{equation}Here, $\mathbf{Z}$ is an $n\times m$ matrix of the (standardized) genotypes of $n$ individuals at $m$ SNPs and $u_j$ is the effect of SNP $j$ on the log-odds for individual $i$.&nbsp; The mixed-model framework assumes the random $u$ come from a normal distribution with mean 0 and standard deviation $\sigma$, so that each variant has but a small effect on disease risk.&nbsp; This theory is known as restricted maximum-likelihood (REML) [@henderson_estimation_1959;@patterson_recovery_1971]
.
Now, the variance of the log-odds $Y$ about its mean $\mathbf{X}\beta$ becomes
\begin{align}
\left(Y-\mathbf{X}\beta\right)\left(Y-\mathbf{X}\beta\right)^T&=\mathbf{Z}uu^T\mathbf{Z}^T+\varepsilon\varepsilon^T\\ &=\left(\mathbf{Z}\mathbf{Z}^T+\mathbf{I}\right)\sigma^2=\mathbf{V}.
(\#eq:3-10)
\end{align}Rearranging and differentiating with respect to $\beta_k$ obtains (with summation over $i$, $j$, and $l$)
\begin{align}
V_{li}^{-1}X_{ik}\left(Y_l-X_{lj}\beta_j\right)^T+V_{il}^{-1}\left(Y_l-X_{lj}\beta_j\right)X^T_{ki}&=0\\\left(X^T_{ki}V_{il}^{-1}Y_l-X^T_{ki}V_{il}^{-1}X_{lj}\beta_j\right)^T+\left(X^T_{ki}V_{il}^{-1}Y_l-X^T_{ki}V_{il}^{-1}X_{lj}\beta_j\right)&=0,
(\#eq:3-11)
\end{align}since $\mathbf{V}$--and hence $\mathbf{V}^{-1}$--is a symmetric matrix.&nbsp; And because the $k$<sup>th</sup> entry of a transposed vector is equal to the $k$<sup>th</sup> entry of the original vector, we get the maximum-likelihood solution
\begin{equation}
\mathbf{X}^T\left(\mathbf{I}+\mathbf{Z}\mathbf{Z}^T\right)^{-1}\mathbf{X}\hat{\beta}=\mathbf{X}^T\left(\mathbf{I}+\mathbf{Z}\mathbf{Z}^T\right)^{-1}Y,
(#eq:3-10)
\end{equation}where $\frac{1}{m}\mathbf{Z}\mathbf{Z}^T$ is the genomic relationship matrix (GRM) we used to compute principle components in [Week 1](#Week1).&nbsp; Thus we can estimatimate the *fixed effects* $\beta$&mdash;including the SNP effect $\beta_1$&mdash;and their standard errors without fitting the *random effects* of every other SNP simultaneously.

## Practice {#Week3_practice}

### Get phenotype {#Week3_practice_pheno}

The first item we shall need when associating SNPs with disease risk is a [phenotypes file](https://github.com/wletsou/BIOL-350/raw/master/docs/EUR_BCa.fam).&nbsp; Download and save this file to the same location as your work from [Week 2](#Week2), along with the (unzipped) [vcf file](https://github.com/wletsou/BIOL-350/raw/master/docs/EUR_BCa.vcf.gz).

Read in the phenotypes file and redname the columns using

```
fam <- read.table("/path/to/file/EUR_BCa.fam",header = FALSE) # import
colnames(fam) <- c("FID","IID","MID","PID","Sex","Phenotype") # name the columns
```

This file is in PLINK format: its first column contains the family id of the individual in the second column; if the samples are unrelated, these two columns will be the same.&nbsp; Similarly, the maternal and paternal ids are the ids of the mother and father of the sample, should they happen to be in the dataset.&nbsp; Sex is left unspecified in this analysis.  The most important column for our purposes is the Phenotype column, which contains a 1 for controls and a 2 for cases.

Complete the steps [KING](#Week2_practice_king), [PC-AiR](#Week2_practice_pcair), and [PC-Relate](#Week2_practice_pcrelate) from [Week 2](#Week2).&nbsp; At the end of this analysis, you should have the results <kbd>mypcair</kbd> and <kbd>mypcrel</kbd> of PC-AiR and PC-Relate, an updated GRM, an open <kbd>GenotypeData</kbd> object called <kbd>genoData</kbd>, and a <kbd>GenotypeBlockIterator</kbd> object called <kbd>genoData.iterator</kbd>.&nbsp; This last will iterate over each of the pruned SNPs included in your model as you test each one for association with the disease phenotype.

The first step is to create a dataframe for the *null model*, which regresses phenotype on genetic ancestry only:

```
mydat <- data.frame(scanID = mypcair$sample.id,pc1 = mypcair$vectors[,1],pc2 = mypcair$vectors[,2],pc3 = mypcair$vectors[,3],pc4 = mypcair$vectors[,4],pc5 = mypcair$vectors[,5],pc6 = mypcair$vectors[,6],pc7 = mypcair$vectors[,7],pc8 = mypcair$vectors[,8],pc9 = mypcair$vectors[,9],pc10 = mypcair$vectors[,10],pheno = fam$Phenotype - 1) # data frame of fixed effects
```

Here we have included the first ten PCs.&nbsp; The last column of this dataframe contains the phenotypes from the <kbd>fam</kbd> file; we have subtracted 1 from each entry so that 0 corresponds to controls, and 1 to cases.

### Fitting the model {#Week3_practice_fit}

Now we will fit the null model, i.e., Eq. \@ref(eq:3-9) with just the fixed covariates $\beta$ and the random SNP effects $u$; no SNP main effects have been fitted yet.&nbsp; The computation is accomplished efficiently in the <kbd>GENESIS</kbd> package [@gogarten_genetic_2019].&nbsp; We need to specify the covariates as columns in dataframe <kbd>mydat</kbd>, and pass the GRM as a covariate matrix:

```
scanAnnot <- ScanAnnotationDataFrame(mydat)
nullmod <- fitNullModel(scanAnnot, outcome = "pheno", covars = c("pc1","pc2","pc3","pc4","pc5","pc6","pc7","pc8","pc9","pc10"), cov.mat = myGRM, family = "binomial") # null model
```

Now using our <kbd>GenotypeDataIterator</kbd> object, we can fit each SNP one-at-a-time:

```
assoc <- assocTestSingle(genoData.iterator,null.model = nullmod) # model including SNPs
```

### Analyzing the results {#Week3_practice_analysis}

The dataframe <kbd>assoc</kbd> will contain a column <kbd>Est</kbd> of log-odds-ratios and another column <kbd>Score.pval</kbd> of p-values.&nbsp; Make a Manhattan plot of the data using:

```
par(mar = c(5.1,5.1,4.1,2.1) ) # left default plus one
plot(assoc$pos,-log10(assoc$Score.pval),xlab = "Position",ylab = "-log10(p)",pch = 19,cex.axis = 1.5,cex.lab = 1.5,cex.main = 1.5) # Manhattan plot
abline(h = 8 - log10(5),col = 'red',lty = 2) # genome-wide significance threshold
close(genoData)
```

Does any SNP appear to be associated with disease?&nbsp; Is is genome-wide significant?

To check that your answer makes sense, extract the data for the most-statistically-significant SNP:

```
assoc[assoc$Score.pval < 1e-5,] # most-significant SNP(s)
```

including its id and association statistics.&nbsp; Then using the <kbd>GWASTools</kbd> <kbd>getVariable</kbd> on your <kbd>genoData</kbd> object, extract the name of this SNP: 

```
getVariable(genoData,"snp.rs.id")[...] # insert the position of the variant in the list
```

To check that the SNP is indeed associated with disease, compute the allele frequency overall, and of the cases and controls separately (being sure to fill in <kbd>'...'</kbd> with the <kbd>variant.id</kbd> you found above):

```
cases_genotypes <- getVariable(genoData,"genotype")[fam$Phenotype == 2,...] # genotypes of cases at the SNP
controls_genotypes <- getVariable(genoData,"genotype")[fam$Phenotype == 1,...] # genotypes of controls at the SNP

p_0 <- sum(getVariable(genoData,"genotype")[,...]) / 2 / length(getVariable(genoData,"genotype")[,...]) # combined allele frequency
p_cases <- sum(cases_genotypes) / 2 / length(cases_genotypes) # cases allele frequency
p_controls <- sum(controls_genotypes) / 2 / length(controls_genotypes) # controls allele frequency
```

Now you can compute a z-score, p-value, and (log) OR:

```
z <- (p_cases - p_controls) / sqrt(p_0 * (1 - p_0) * (1 / (length(cases_genotypes) - 1) + 1 / (length(controls_genotypes) - 1))) # z-score for allele frequency difference

pnorm(-abs(z),lower.tail = TRUE) + pnorm(abs(z),lower.tail = FALSE) # two-sided p-value

log( p_cases * (1 - p_controls) / (1 - p_cases) / p_controls) # log-OR
```

How does your answer compare with the adjusted p-value found in the <kbd>assoc</kbd> table?&nbsp; What might account for the difference?

### To turn in: {#Week3_practice_turnin}

1. A Manhattan plot of the SNPs in the region
2. Information extracted from the <kbd>assoc</kbd> dataframe about the most-significant SNP
3. Calculation of the crude logOR and p-value of the most-significant SNP
4. Population genetic information, located in the [enselmbl genome browser](https://useast.ensembl.org/Homo_sapiens/Info/Index): what is the frequency in the European population of the allele you identified?

## References {#Week3_ref}